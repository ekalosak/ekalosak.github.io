<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" type="text/css" href="style.css" media="screen" />
    <title>Everything's a graph</title>
  </head>
<body>

<h1 id="everything-s-a-graph">Everything's a graph</h1>

My name's Eric. Welcome to my blog.

<h2>October 31, 2022</h2>

Happy Halloween!

<h3>Braiiiiins</h3>

<h1> TODO </h1>

<ol>
<li>brain obj mesh pyvista nx graph with pos</li>
<li>life on brain</li>
</ol>

<h2>October 30, 2022</h2>

Hi Mom :) Dad!! Look I'm on the internet <3

<h3>Graphs can learn graphs</h3>
Models here are represented as graphs of modules.

<h3>Introduction</h3>
Shchur et al. 2019
<a href="https://arxiv.org/abs/1811.05868">arXiv:1811.05868</a>
provide a clear-eyed set of pitfalls to avoid when developing with graph neural nets. Two prominent points stand out:

<ol>
<li>Sometimes smaller nets do better</li>
<li>Remember to replicate and cross validate</li>
</ol>

Here I'll implement two of Shchur's baseline models along with an even simpler one, train them on a linear process over
a small graph, and show evidence in support of the first point using the second.

<h3>Models</h3>

On the first point, I'll endeavour to fit the smallest capable models possible among three basic classes of models:
<ol>
<li>Linear model</li>
<li>Single-layer fully connected neural net</li>
<li>Single-layer graph convolutional neural net</li>
</ol>

The neural nets each have a single layer and feed forward into a relu activation function.
The GCN is from Kipf and Welling 2016 <a href="https://arxiv.org/abs/1609.02907">arXiv:1609.02907</a>.
As the RGBA values are all in [0,1], neither input nor output embeddings are required.  None of the models include an
extra bias term because the alpha channel is 1 everywhere.

<h3>Code</h3>
The Python code for each model is reproduced below:
<pre>
<code>
class LM(torch.nn.Module):
    """ y = mx + b, simple as that """
    def __init__(self, channels=4, bias=True):
        super().__init__()
        self.lin0 = Linear(channels, channels, bias=bias)

    def forward(self, data):
        x = data.x
        x = self.lin0(x)
        return x
</code>
</pre>

<pre>
<code>
class MLP(torch.nn.Module):
    """ multi-layer perceptron with a single layer """
    def __init__(self, channels=4, bias=True):
        super().__init__()
        self.lin0 = Linear(channels, channels, bias=bias)

    def forward(self, data):
        x = data.x
        x = self.lin0(x)
        x = F.relu(x)
        return x
</code>
</pre>

<pre>
<code>
class GCN(torch.nn.Module):
    """ Kipf and Welling 2017 eqn (9) """
    def __init__(self, channels=4, bias=True):
        super().__init__()
        self.conv0 = GCNConv(channels, channels, bias=bias)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv0(x, edge_index)
        x = F.relu(x)
        return x
</code>
</pre>

The data objects are instances of torch_geometric.Data and F is torch.nn.functional.

<h3>Data</h3>
For a graph G with degree matrix D and adjacency matrix A, a linear diffusion process here is defined recursively as
Xt+1 = (1-a)Xt + aLXt. The scalar a is the diffusion rate and L = A - D is the graph Laplacian. Each Xt is a 4-channel
2-tensor holding the RGBA values for each node's color - the shape is (5, 4). The alpha channel is held constant at 1
and the three channels are initialized independently from U[0,1]. After 8 steps, the colors are re-initialized.

<img
  src="static/diffusion-train-data.gif"
  alt="raw diffusion training data deltas render"
  class="center"
/>
<br>


The dataset consists of 16,384 observations (x=Xt, y=Xt+1). The average color change âˆ†=sum(|x-y|)/len(G) between steps
are plotted below:

<img
  src="static/diffusion-delta-16384.png"
  alt="raw diffusion training data deltas hist"
  class="center"
/>
<br>

The raw observations in this dataset are heavily skewed towards smaller changes - in the second half of the simulation,
most colors are similar. There's a bump at 0.07 - those observations are the re-initializations - they are removed in
the processed dataset.

<h3>Training</h3>

<h4>Visualization of model snapshots</h4>
The following gifs show GCN model's RBG predictions over the course of training. The alpha channel is fixed at 1.

<h3>gcn before any training</h3>
The model parameters are initialized near zero - the following are model predictions from the GCN before training:
<img
  src="static/diffusion-e0-gcn.gif"
  alt="gcn at epoch 0"
  class="center"
  style="width:100%"
/>

<h3>gcn after 24 of 96 epochs</h3>
<img
  src="static/diffusion-e24-gcn.gif"
  alt="gcn at epoch 24"
  class="center"
  style="width:100%"
/>

<h3>gcn after 96 epochs</h3>
<img
  src="static/diffusion-e96-gcn.gif"
  alt="gcn at epoch 96"
  class="center"
  style="width:100%"
/>


<h4>Results</h4>

The spaghetti plots below show the train and test loss falling as the models each learn the dataset.
8-fold cross validation is used and each run is replicated 4 times, resulting in 32 noodles for each model. The
test error is calculated 12 times over the course of 96 training epochs, averaged over each element of the validation
set. No optimizer hyperparameters are systematically tuned; I just picked adam with lr=0.02 and wd=5e-4 after some
tinkering.

<h3>linear model</h3>
<img
  src="static/diffusion-train-lm.png"
  alt="spaghetti linear bull"
  class="center"
  style="width:100%"
/>
<br>


<h3>multi-layer perceptron</h3>
<img
  src="static/diffusion-train-mlp.png"
  alt="spaghetti mlp bull"
  class="center"
  style="width:100%"
/>
<br>


<h3>graph convolutional layer</h3>
<img
  src="static/diffusion-train-gcn.png"
  alt="spaghetti gcn bull"
  class="center"
  style="width:100%"
/>
<br>

<h3>Summary</h3>

Train and test error appear correlated; I haven't plotted the blocked replication and train/test split
effects, but on the whole, for 87.5% of the training runs,
<ul>
<li>The training MSE fell to [0.006, 0.02] using the linear model.</li>
<li>The training MSE fell to [0.004, 0.18] using the multi-layer perceptron.</li>
<li>The training MSE fell to [0.017, 0.03] using the graph convolutional layer.</li>
</ul>

In this baseline case, the best relational inductive bias is the naieve one.

<h3>See also</h3>
<ul>
<li>Wu, Pan et al. 2019 <a href="https://arxiv.org/abs/1901.00596">arXiv:1901.00596</a></li>
<li>Battaglia, Haprick et al. 2018 <a href="https://arxiv.org/abs/1806.01261">arXiv:1806.01261</a></li>
</ul>

<h3>Appendix</h3>
The full set of experimental parameters are captured here:
<pre>
<code>
dataset_name = "diffusion"
graph_name = "bull"
python = "3.10.6 (main, Sep 14 2022, 08:30:16) [Clang 10.0.1 (clang-1001.0.46.4)]"
platform = "darwin"
implementation = "namespace(name='cpython', cache_tag='cpython-310', version=sys.version_info(major=3, minor=10, micro=6, releaselevel='final', serial=0), hexversion=50988784, _multiarch='darwin')"

[config]
uid = "e4dd0505-758d-492a-89b3-94c08f21344e"
updated = 2022-10-28T17:02:00.154517
root = "data/diffusion"
name = "small graph diffusion baseline"

[[config.modules]]
kind = "gcn"
[[config.modules.kwargs]]
bias = false


[[config.modules]]
kind = "lm"
[[config.modules.kwargs]]
bias = false


[[config.modules]]
kind = "mlp"
[[config.modules.kwargs]]
bias = false

[[config.datasets]]
kind = "diffusion"
graph = "bull"
nobs = 16384
force = false
[[config.datasets.kwargs]]
nstep = 8
diffusion_rate = 0.2
force = false

[config.train]
epochs = 96
checkpoints = 4
n_test_eval = 12
nsplits = 8
nreps = 4

[config.train.optimizer]
kind = "adam"

[config.train.loader]
batch_size = 64
shuffle = false

[config.train.optimizer.kwargs]
lr = 0.02
weight_decay = 0.0005
</code>
</pre>

<h2>Older posts</h2>
<h3 id="2022-10-12"><a href="blog2.html">October 12, 2022</a></h3>
<h3 id="2022-10-02"><a href="blog1.html">October 2, 2022</a></h3>
<h3 id="2022-10-01"><a href="blog0.html">October 1, 2022</a></h3>

</body>
</html>
